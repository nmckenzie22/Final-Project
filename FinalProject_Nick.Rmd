---
title: "FinalProjectCode"
output: html_document
date: "2025-05-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load required libraries
library(readr)
library(dplyr)
#install.packages("janitor") 
library(janitor)
library(httr)
library(tidyr)
library(ggplot2)
#install.packages("caret")
library(caret)
#install.packages("corrplot")
library(corrplot)
#install.packages("GGally")
library(GGally)
#install.packages("forcats")
library(forcats)

# Read CSV directly from GitHub raw link
url <- "https://raw.githubusercontent.com/nmckenzie22/Final-Project/main/US%20counties%20-%20education%20vs%20per%20capita%20personal%20income%20-%20results-20221227-213216.csv"
data <- read_csv(url)

# Clean column names
data <- clean_names(data)

# Peek at data
glimpse(data)

```

### Variable Summary Table

```{r}
library(tibble)

variable_summary <- tibble(
  Variable = colnames(data),
  Type = c(
    "Nominal",  # county_fips
    "Nominal",  # state
    "Nominal",  # county
    "Ratio",    # per_capita_personal_income_2019
    "Ratio",    # per_capita_personal_income_2020
    "Ratio",    # per_capita_personal_income_2021
    "Ratio",    # associate_degree_numbers_2016_2020
    "Ratio",    # bachelor_degree_numbers_2016_2020
    "Ratio",    # associate_degree_percentage_2016_2020
    "Ratio"     # bachelor_degree_percentage_2015_2019
  ),
  Description = c(
    "Federal Information Processing Standards (FIPS) code uniquely identifying the county.",
    "Two-letter abbreviation of the U.S. state.",
    "Name of the U.S. county.",
    "Per capita personal income in 2019 (in USD).",
    "Per capita personal income in 2020 (in USD).",
    "Per capita personal income in 2021 (in USD).",
    "Number of residents with an associate degree between 2016 and 2020.",
    "Number of residents with a bachelor’s degree between 2016 and 2020.",
    "Percentage of residents with an associate degree between 2016 and 2020.",
    "Percentage of residents with a bachelor’s degree between 2015 and 2019."
  )
)

print(variable_summary)

```

### EDA
Answering question: How does Education affect income?
First we look at the summary statistics:

```{r}
summary(data)
```

Missing values check:
```{r}
colSums(is.na(data))
```

Income growth over time:
```{r}
library(ggplot2)

data %>%
  pivot_longer(cols = starts_with("per_capita_personal_income"),
               names_to = "year",
               names_prefix = "per_capita_personal_income_",
               values_to = "income") %>%
  ggplot(aes(x = year, y = income)) +
  geom_boxplot(fill = "skyblue") +
  labs(title = "Distribution of Per Capita Income by Year",
       y = "Income (USD)", x = "Year") +
  theme_minimal()
```

Education percentages:
```{r}
data %>%
  select(associate_degree_percentage_2016_2020, bachelor_degree_percentage_2015_2019) %>%
  pivot_longer(everything(), names_to = "degree_type", values_to = "percentage") %>%
  ggplot(aes(x = degree_type, y = percentage, fill = degree_type)) +
  geom_boxplot() +
  labs(title = "Education Level Distribution", y = "Percentage", x = "Degree Type") +
  theme_minimal()
```

Scatter Plots: Education vs. Income (2021):
```{r}
ggplot(data, aes(x = associate_degree_percentage_2016_2020, y = per_capita_personal_income_2021)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(title = "Associate Degree % vs. Per Capita Income (2021)",
       x = "Associate Degree Percentage",
       y = "Per Capita Income 2021") +
  theme_minimal()

ggplot(data, aes(x = bachelor_degree_percentage_2015_2019, y = per_capita_personal_income_2021)) +
  geom_point(alpha = 0.5, color = "forestgreen") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(title = "Bachelor's Degree % vs. Per Capita Income (2021)",
       x = "Bachelor's Degree Percentage",
       y = "Per Capita Income 2021") +
  theme_minimal()
```

Correlation Matrix:
```{r}
library(corrplot)

numeric_data <- data %>%
  select(where(is.numeric)) %>%
  na.omit()

cor_matrix <- cor(numeric_data)

corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8)

```

Linear Assumptions & Distribution Checks (This checks: Normality of variables, Linearity, Outliers):
```{r}
ggpairs(numeric_data[, c("per_capita_personal_income_2021", 
                         "associate_degree_percentage_2016_2020", 
                         "bachelor_degree_percentage_2015_2019")])

```

### Training machine learning model

Preprocessing for Modeling:
```{r}
# Handling missing values by imputing or removing rows with NA values (based on strategy)
# We will use median imputation for numeric variables and remove any rows with missing categorical values.
data_clean <- data %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
  drop_na(state, county)  # Removing rows with missing categorical variables

# Check again for missing values
colSums(is.na(data_clean))

# Scaling numeric variables (important for models like linear regression)
numeric_data <- data_clean %>%
  select(where(is.numeric)) %>%
  scale()

# Combine the scaled numeric data with the categorical data
data_preprocessed <- cbind(data_clean %>%
                              select(state, county, county_fips), numeric_data)
```

Feature Selection:
Now let’s define the target variable as per_capita_personal_income_2021. We'll use the other features as predictors.
```{r}
# Define the target variable (dependent) and predictor variables (independent)
target_variable <- "per_capita_personal_income_2021"
predictors <- setdiff(colnames(data_preprocessed), target_variable)

# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)
train_index <- createDataPartition(data_preprocessed[[target_variable]], p = 0.7, list = FALSE)
train_data <- data_preprocessed[train_index, ]
test_data <- data_preprocessed[-train_index, ]
```

Train a Machine Learning Model:
For simplicity, we'll start with a linear regression model:

```{r}
# Train a linear regression model
lm_model <- lm(as.formula(paste(target_variable, "~", paste(predictors, collapse = "+"))), data = train_data)

# Summary of the model
summary(lm_model)
```

Model Evaluation:
```{r}
# Combine train and test datasets
combined_data <- bind_rows(train_data, test_data)

# Ensure 'county_fips' has the same levels in both train and test data
combined_data$county_fips <- factor(combined_data$county_fips)

# Now split the combined dataset back into training and test datasets
train_data <- combined_data[1:nrow(train_data), ]
test_data <- combined_data[(nrow(train_data) + 1):nrow(combined_data), ]

# Remove 'county' from the predictor variables (optional, if you don't need it in the model)
predictors <- setdiff(predictors, "county")
# Remove 'county_fips' from predictors if it is not needed in the model
predictors <- setdiff(predictors, "county_fips")

# Train a linear regression model
lm_model <- lm(as.formula(paste(target_variable, "~", paste(predictors, collapse = "+"))), data = train_data)

# Summary of the model
summary(lm_model)

# Predict on the test set
predictions <- predict(lm_model, newdata = test_data)

# Calculate performance metrics
actuals <- test_data[[target_variable]]
residuals <- actuals - predictions

# RMSE (Root Mean Squared Error)
rmse <- sqrt(mean(residuals^2))

# MAE (Mean Absolute Error)
mae <- mean(abs(residuals))

# R-squared
rsq <- 1 - sum(residuals^2) / sum((actuals - mean(actuals))^2)

# Print evaluation metrics
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("R-squared:", rsq, "\n")

```

