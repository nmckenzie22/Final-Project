---
title: "FinalProjectCode"
output: html_document
date: "2025-05-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load required libraries
library(readr)
library(dplyr)
#install.packages("janitor") 
library(janitor)
library(httr)
library(tidyr)
library(ggplot2)
#install.packages("caret")
library(caret)
#install.packages("corrplot")
library(corrplot)
#install.packages("GGally")
library(GGally)
#install.packages("forcats")
library(forcats)

# Read CSV directly from GitHub raw link
url <- "https://raw.githubusercontent.com/nmckenzie22/Final-Project/main/US%20counties%20-%20education%20vs%20per%20capita%20personal%20income%20-%20results-20221227-213216.csv"
data <- read_csv(url)

# Clean column names
data <- clean_names(data)

# Peek at data
glimpse(data)

```

### Variable Summary Table

```{r}
library(tibble)
library(tibble)
library(gridExtra)
library(grid)

variable_summary <- tibble(
  Variable = colnames(data),
  Type = c(
    "Nominal",  # county_fips
    "Nominal",  # state
    "Nominal",  # county
    "Ratio",    # per_capita_personal_income_2019
    "Ratio",    # per_capita_personal_income_2020
    "Ratio",    # per_capita_personal_income_2021
    "Ratio",    # associate_degree_numbers_2016_2020
    "Ratio",    # bachelor_degree_numbers_2016_2020
    "Ratio",    # associate_degree_percentage_2016_2020
    "Ratio"     # bachelor_degree_percentage_2015_2019
  ),
  Description = c(
    "Federal Information Processing Standards (FIPS) code uniquely identifying the county.",
    "Two-letter abbreviation of the U.S. state.",
    "Name of the U.S. county.",
    "Per capita personal income in 2019 (in USD).",
    "Per capita personal income in 2020 (in USD).",
    "Per capita personal income in 2021 (in USD).",
    "Number of residents with an associate degree between 2016 and 2020.",
    "Number of residents with a bachelor’s degree between 2016 and 2020.",
    "Percentage of residents with an associate degree between 2016 and 2020.",
    "Percentage of residents with a bachelor’s degree between 2015 and 2019."
  )
)
# Save table as image
png("variable_summary_table.png", width = 1200, height = 500)
grid.table(variable_summary)
dev.off()

print(variable_summary)

```

### EDA
Answering question: How does Education affect income?
First we look at the summary statistics:

```{r}
summary(data)
```

Missing values check:
```{r}
colSums(is.na(data))
```

Income growth over time:
```{r}
library(ggplot2)
library(scales)
library(dplyr)
library(tidyr)

data %>%
  pivot_longer(
    cols = starts_with("per_capita_personal_income"),
    names_to = "year",
    names_prefix = "per_capita_personal_income_",
    values_to = "income"
  ) %>%
  ggplot(aes(x = year, y = income)) +
  geom_boxplot(fill = "skyblue") +
  labs(
    title = "Distribution of Per Capita Income by Year",
    y = "Income (USD)",
    x = "Year"
  ) +
  scale_y_continuous(labels = label_comma()) +  # <-- formats y-axis numbers
  theme_minimal()

# Save to file
ggsave(
  filename = "~/Downloads/distribution_income_byYear.png",
  plot = last_plot(),
  width = 7,
  height = 4,
  dpi = 300
)
```

Education percentages:
```{r}
plot <- data %>%
  select(associate_degree_percentage_2016_2020, bachelor_degree_percentage_2015_2019) %>%
  pivot_longer(everything(), names_to = "degree_type", values_to = "percentage") %>%
  ggplot(aes(x = degree_type, y = percentage, fill = degree_type)) +
  geom_boxplot() +
  labs(
    title = "Education Level Distribution",
    y = "Percentage"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

# Display plot
print(plot)

# Save to file
ggsave(
  filename = "~/Downloads/education_level_boxplot.png",
  plot = last_plot(),
  width = 7,
  height = 4,
  dpi = 300
)
```

Scatter Plots: Education vs. Income (2021):
```{r}
library(ggplot2)
library(scales)

# First plot: Associate Degree %
p1 <- ggplot(data, aes(x = associate_degree_percentage_2016_2020, y = per_capita_personal_income_2021)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(
    title = "Associate Degree % vs. Per Capita Income (2021)",
    x = "Associate Degree Percentage",
    y = "Per Capita Income 2021"
  ) +
  scale_y_continuous(labels = label_comma()) +
  theme_minimal()

print(p1)

# Save to Downloads (macOS)
ggsave(
  filename = "~/Downloads/assoc_deg_vs_income.png",
  plot = p1,
  width = 8,
  height = 3,
  dpi = 300
)


# Second plot: Bachelor's Degree %
p2 <- ggplot(data, aes(x = bachelor_degree_percentage_2015_2019, y = per_capita_personal_income_2021)) +
  geom_point(alpha = 0.5, color = "forestgreen") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(
    title = "Bachelor's Degree % vs. Per Capita Income (2021)",
    x = "Bachelor's Degree Percentage",
    y = "Per Capita Income 2021"
  ) +
  scale_y_continuous(labels = label_comma()) +
  theme_minimal()

print(p2)

# Save to Downloads (macOS)
ggsave(
  filename = "~/Downloads/bach_deg_vs_income.png",
  plot = p2,
  width = 6,
  height = 3,
  dpi = 300
)

```

Correlation Matrix:
```{r}
library(corrplot)
library(dplyr)
library(stringr)

# Clean and rename numeric columns
numeric_data <- data %>%
  select(where(is.numeric)) %>%
  na.omit()

# Create readable, titled column names
clean_names <- colnames(numeric_data) %>%
  str_replace_all("_", " ") %>%
  str_to_title()

colnames(numeric_data) <- clean_names

# Compute correlation matrix
cor_matrix <- cor(numeric_data)

# --- 1. Print to RStudio viewer ---
corrplot(
  cor_matrix,
  method = "color",
  type = "upper",
  tl.cex = 0.8,
  addCoef.col = "black",
  number.cex = 0.7,
  col = colorRampPalette(c("red", "white", "blue"))(200)
)

# --- 2. Save to Downloads folder ---
png(filename = "~/Downloads/correlation_plot.png", width = 800, height = 800)

corrplot(
  cor_matrix,
  method = "color",
  type = "upper",
  tl.cex = 0.8,
  addCoef.col = "black",
  number.cex = 0.7,
  col = colorRampPalette(c("red", "white", "blue"))(200)
)

dev.off()

```

Linear Assumptions & Distribution Checks (This checks: Normality of variables, Linearity, Outliers):
```{r}
library(GGally)

# Convert matrix to data frame
numeric_df <- as.data.frame(numeric_data)

# Rename columns directly (overwrite column names in place)
colnames(numeric_df)[colnames(numeric_df) == "per_capita_personal_income_2021"] <- "Per Capita Income 2021"
colnames(numeric_df)[colnames(numeric_df) == "associate_degree_percentage_2016_2020"] <- "Associate Degree % (2016–2020)"
colnames(numeric_df)[colnames(numeric_df) == "bachelor_degree_percentage_2015_2019"] <- "Bachelor's Degree % (2015–2019)"

# Create default ggpairs plot (retaining standard look)
pair_plot <- ggpairs(numeric_df[, c(
  "Per Capita Income 2021",
  "Associate Degree % (2016–2020)",
  "Bachelor's Degree % (2015–2019)"
)])

# Save to Downloads
ggsave(
  filename = "~/Downloads/clean_pair_plot.png",
  plot = pair_plot,
  width = 6.5,
  height = 6.5,
  dpi = 300
)
```

### Training machine learning model

Preprocessing for Modeling:
```{r}
# Handling missing values by imputing or removing rows with NA values (based on strategy)
# We will use median imputation for numeric variables and remove any rows with missing categorical values.
data_clean <- data %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
  drop_na(state, county)  # Removing rows with missing categorical variables

# Check again for missing values
colSums(is.na(data_clean))

# Scaling numeric variables (important for models like linear regression)
numeric_data <- data_clean %>%
  select(where(is.numeric)) %>%
  scale()

# Combine the scaled numeric data with the categorical data
data_preprocessed <- cbind(data_clean %>%
                              select(state, county, county_fips), numeric_data)
```

Feature Selection:
Now let’s define the target variable as per_capita_personal_income_2021. We'll use the other features as predictors.
```{r}
# Define the target variable (dependent) and predictor variables (independent)
target_variable <- "per_capita_personal_income_2021"
predictors <- setdiff(colnames(data_preprocessed), target_variable)

# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)
train_index <- createDataPartition(data_preprocessed[[target_variable]], p = 0.7, list = FALSE)
train_data <- data_preprocessed[train_index, ]
test_data <- data_preprocessed[-train_index, ]
```

Train a Machine Learning Model:
For simplicity, we'll start with a linear regression model:

```{r}
# Train a linear regression model
lm_model <- lm(as.formula(paste(target_variable, "~", paste(predictors, collapse = "+"))), data = train_data)

# Summary of the model
summary(lm_model)
```

Model Evaluation:
```{r}
# Combine train and test datasets
combined_data <- bind_rows(train_data, test_data)

# Ensure 'county_fips' has the same levels in both train and test data
combined_data$county_fips <- factor(combined_data$county_fips)

# Now split the combined dataset back into training and test datasets
train_data <- combined_data[1:nrow(train_data), ]
test_data <- combined_data[(nrow(train_data) + 1):nrow(combined_data), ]

# Remove 'county' from the predictor variables (optional, if you don't need it in the model)
predictors <- setdiff(predictors, "county")
# Remove 'county_fips' from predictors if it is not needed in the model
predictors <- setdiff(predictors, "county_fips")

# Train a linear regression model
lm_model <- lm(as.formula(paste(target_variable, "~", paste(predictors, collapse = "+"))), data = train_data)

# Summary of the model
summary(lm_model)

# Predict on the test set
predictions <- predict(lm_model, newdata = test_data)

# Calculate performance metrics
actuals <- test_data[[target_variable]]
residuals <- actuals - predictions

# RMSE (Root Mean Squared Error)
rmse <- sqrt(mean(residuals^2))

# MAE (Mean Absolute Error)
mae <- mean(abs(residuals))

# R-squared
rsq <- 1 - sum(residuals^2) / sum((actuals - mean(actuals))^2)

# Print evaluation metrics
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("R-squared:", rsq, "\n")

```

